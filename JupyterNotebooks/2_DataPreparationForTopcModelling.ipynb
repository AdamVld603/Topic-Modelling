{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armad\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\Armad\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import regex\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_cleaned_1864_test.csv', '_cleaned_1900_test.csv']\n"
     ]
    }
   ],
   "source": [
    "# --- Read Cleaned and Preprcessed data:\n",
    "\n",
    "# Provide Location of cleaned csv's:\n",
    "data_location_cleaned = '.\\\\data_cleaned_test\\\\' #Change path to where original .csv's are\n",
    "my_directory_cleaned = os.listdir(data_location_cleaned) \n",
    "print(my_directory_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Version: 2.1.8\n"
     ]
    }
   ],
   "source": [
    "# Load spacy with Einglish language model\n",
    "\n",
    "# import spacy for Natural Language Processing (language: English)\n",
    "#import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "print('spaCy Version: %s' % (spacy.__version__))\n",
    "#spacy.explain('VBD') #'VBP'\n",
    "#spacy.explain('VBN')\n",
    "#test=nlp(u\"TeSt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a single stopword:\n",
    "#nlp.Defaults.stop_words.add(\"nan\") \n",
    "\n",
    "# Check pre-defined stop words in spacy:\n",
    "#print(nlp.Defaults.stop_words)\n",
    "\n",
    "# Check if particular word is a stopward:\n",
    "#print(nlp.vocab[\"nan\"].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doing', 'forty', 'he', 'already', 'him', 'call', 'thence', 'say', 'its', 'whatever', 'n‘t', 'other', 'against', 'above', \"'ve\", 'via', 'wherein', 'became', 'moreover', 'too', 'itself', 'somewhere', 'herein', 'beforehand', 'nor', 'amount', 'towards', \"'s\", 'a', 'part', '‘re', 'most', 'perhaps', 'thereafter', 'take', 'show', 'please', 'yourselves', 'mine', 'within', 'few', 'go', 'be', 'bottom', 'per', 'cannot', 'whereafter', 'her', 'who', 'we', 'much', 'each', 'besides', 'thereby', 'side', 'everywhere', 'nevertheless', 'hence', 'beside', 'they', 'in', 'hereafter', 'hers', 'an', 'has', 'been', 'less', 'made', 'others', 'still', 'were', 'should', 'have', 'from', 'wherever', 'eleven', 'formerly', 'until', 'just', 'along', 'as', 'being', 'first', 'using', 'hereby', 'sixty', 'anyone', 'becomes', 'although', 'six', 'these', 'anyway', 'further', 'down', 'out', \"'ll\", 'used', 'since', 'least', 'this', 'otherwise', 'sometime', 'either', 'may', 'noone', \"'re\", 'into', 'whenever', 'same', '’s', 'now', 'will', 'can', 'whose', 'what', 'full', 'on', 'put', 'if', 'give', 'throughout', 'fifteen', 'see', 'three', 'unless', 'any', 'seeming', '’re', 'those', 'ourselves', 're', 'afterwards', 'fifty', 'did', 'every', 'whither', 'somehow', 'before', 'keep', 'elsewhere', 'everyone', 'many', 'done', \"'d\", 'move', 'namely', 'various', '‘s', 'alone', 'more', 'your', 'or', 'would', 'might', 'therein', 'it', 'his', 'for', 'ca', 'once', 'through', 'one', 'own', 'off', 'never', '‘ve', 'that', 'none', 'thereupon', 'latter', 'meanwhile', 'am', 'eight', 'very', 'name', 'themselves', 'indeed', 'nowhere', 'whole', 'seems', 'here', 'whereupon', 'whether', 'how', 'latterly', 'nobody', 'us', 'after', \"'m\", 'anyhow', 'almost', 'up', 'where', 'under', 'often', \"n't\", 'at', 'beyond', 'had', 'really', 'neither', 'i', 'with', 'last', 'about', 'whence', 'empty', 'something', 'though', 'below', '’ve', 'enough', 'are', 'next', 'n’t', '’d', 'them', 'again', 'four', 'twenty', 'well', 'together', 'both', 'ever', 'back', 'yet', 'five', 'so', 'himself', 'become', 'hereupon', 'make', 'by', 'during', 'some', 'yourself', 'two', 'get', 'you', 'several', '‘ll', 'is', 'hundred', 'rather', 'twelve', 'amongst', 'mostly', 'and', '’m', 'there', 'onto', 'our', 'thru', 'to', 'seemed', 'myself', 'even', 'ten', 'front', 'which', 'their', 'could', 'sometimes', 'someone', 'such', 'thus', 'across', 'she', 'must', 'whereby', 'why', 'without', 'was', '‘d', 'serious', 'upon', 'anything', 'top', 'no', 'around', 'anywhere', 'only', 'among', 'becoming', 'does', 'nothing', 'behind', 'everything', 'however', 'the', '’ll', 'between', 'therefore', 'always', 'do', 'else', 'yours', 'whom', 'over', 'former', 'regarding', 'another', 'ours', 'seem', 'when', 'nine', 'because', 'all', 'quite', 'due', 'than', 'me', 'third', 'whoever', 'of', 'but', '‘m', 'my', 'herself', 'then', 'except', 'not', 'toward', 'whereas', 'while', 'also'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Spacy's STOP_WORDS for English language:\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#Check the STOP_WORDS:\n",
    "print(STOP_WORDS) \n",
    "len(STOP_WORDS)\n",
    "nlp.vocab[\"yet\"].is_stop # Checking If A Word is a Stopword "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding custom \"stop-words\":\n",
    "*  For example, because we analyze a newspapers corpus it make sense to remove words wich are common in newspapers as \"today\", \"Mr\", \"Ms\", \"said\", etc. as they don't have significant value for topic modelling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my custome stopwords:\n",
    "custom_stopwords = {\"the\", \"it\"}\n",
    "\n",
    "# Combine my stopwords with Spacy's stopwords\n",
    "my_stopwords=STOP_WORDS|custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to Preproscess our cleaned data \n",
    "# for use in Topic Modelling\n",
    "\n",
    "def preproscess_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    punctuations = string.punctuation\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        #tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = [tok for tok in tokens if tok not in my_stopwords]        \n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year:  _cleaned_1864_test.csv  ...\n",
      "Processed 1000 out of 2998 documents.\n",
      "Processed 2000 out of 2998 documents.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processed Time for:  _cleaned_1864_test.csv --> 73.08807158470154\n",
      "Saving processed data ... \n",
      "Processing year:  _cleaned_1900_test.csv  ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processed Time for:  _cleaned_1900_test.csv --> 5.750996112823486\n",
      "Saving processed data ... \n",
      "****************************************************************************************************\n",
      "Total time:  79.18606686592102\n"
     ]
    }
   ],
   "source": [
    "# === Preprocess and prepare our cleaned data \n",
    "#  and save it in new csv's, for later use in Topic Modeling:\n",
    "\n",
    "processing_start_time_total = time.time()\n",
    "i=0\n",
    "for y in my_directory_cleaned: \n",
    "    processing_start_time_year = time.time()\n",
    "    doc_text = pd.read_csv(data_location_cleaned + y, encoding='utf-8', dtype=str) #header = None)\n",
    "    #doc_text  = doc_text[doc_text['article'].notnull()]   # Removesw null values\n",
    "    doc_text = doc_text.dropna(how='any',axis=0) # Removesw null values\n",
    "    \n",
    "    #Combine \"article\" and \"title\" columns: [OPTIONAL] \n",
    "    doc_text[\"article\"] = doc_text[\"title\"].map(str) + \" \" + doc_text[\"article\"].map(str)    \n",
    "    #print(doc_text.head())\n",
    "    \n",
    "    print(\"Processing year: \", str(y), \" ...\")\n",
    "    cleaned_= preproscess_text(doc_text[\"article\"], logging=True) \n",
    "    doc_text = doc_text.reset_index(drop=True)\n",
    "    doc_text['processed'] = cleaned_\n",
    "    \n",
    "    processing_end_time_year = time.time()\n",
    "    print(\"-\"*100)\n",
    "    print(\"Processed Time for: \", str(y), \"-->\", processing_end_time_year-processing_start_time_year)\n",
    "    \n",
    "    #Save Preproscessed Data to csv:\n",
    "    print(\"Saving processed data ... \")\n",
    "    doc_text['article'] = doc_text['processed']\n",
    "    del doc_text['processed']\n",
    "    title_=y\n",
    "    title_ = title_.replace('_cleaned_', '')\n",
    "    title_ = title_.replace('.csv', '')\n",
    "    path_cleaned =  '.\\\\data_cleaned_processed\\\\'\n",
    "    doc_text.to_csv(path_cleaned + '_cleaned_'+'spacy_'+title_+'.csv', index = None)#, header = False)    \n",
    "    \n",
    "    \n",
    "processing_end_time_total = time.time()\n",
    "print(\"*\"*100)\n",
    "print(\"Total time: \", processing_end_time_total-processing_start_time_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(doc_text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
