{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Armad\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\Armad\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import regex\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 ---Preprocess/clean all origanal .csv's and store them in one .csv [Start]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1864.csv', '1900.csv']\n"
     ]
    }
   ],
   "source": [
    "# Read Datasets\n",
    "data_location = '.\\\\data_original\\\\' #Change path to where original .csv's are\n",
    "my_directory = os.listdir(data_location) \n",
    "print(my_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Data Preprocessing/Cleaning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: DeprecationWarning: invalid escape sequence \\/\n",
      "<>:22: DeprecationWarning: invalid escape sequence \\/\n",
      "<>:21: DeprecationWarning: invalid escape sequence \\/\n",
      "<>:22: DeprecationWarning: invalid escape sequence \\/\n",
      "<>:21: DeprecationWarning: invalid escape sequence \\/\n",
      "<>:22: DeprecationWarning: invalid escape sequence \\/\n",
      "<ipython-input-11-2e0b68a8f650>:21: DeprecationWarning: invalid escape sequence \\/\n",
      "  df['article'] = df['article'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])# Data preprocessing --> remove URL\n",
      "<ipython-input-11-2e0b68a8f650>:22: DeprecationWarning: invalid escape sequence \\/\n",
      "  df['title'] = df['title'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])# Data preprocessing --> remove URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  1864.csv\n",
      "Data preprocessing --> Removing URL's ...\n",
      "Data preprocessing --> Convert all to lower case ...\n",
      "Data preprocessing --> Removing Punctuation ...\n",
      "Data preprocessing --> Removing Numbers ...\n",
      "Data preprocessing --> Removing non-latin characters ...\n",
      "Data preprocessing --> Removing StopWords ...\n",
      "Data preprocessing --> Stemming ...\n",
      "Re-join words after Tokenization ...\n",
      "Saving:  1864 .csv\n",
      "Done! --> Rows processed:  15829\n",
      "Completed:  1864 --> Time:  70.89200234413147\n",
      "==========\n",
      "Processing:  1900.csv\n",
      "Data preprocessing --> Removing URL's ...\n",
      "Data preprocessing --> Convert all to lower case ...\n",
      "Data preprocessing --> Removing Punctuation ...\n",
      "Data preprocessing --> Removing Numbers ...\n",
      "Data preprocessing --> Removing non-latin characters ...\n",
      "Data preprocessing --> Removing StopWords ...\n",
      "Data preprocessing --> Stemming ...\n",
      "Re-join words after Tokenization ...\n",
      "Saving:  1900 .csv\n",
      "Done! --> Rows processed:  69609\n",
      "Completed:  1900 --> Time:  31.34299898147583\n",
      "==========\n",
      "**********\n",
      "Data Preprocessing/Cleaning Done!   --> Total Time:  102.2799973487854\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocess/clean original data:\n",
    "\n",
    "fields = ['article', 'title']\n",
    "\n",
    "# Dictionary of nltk's stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "start_time_total = time.time()\n",
    "\n",
    "for year_ in my_directory:\n",
    "    chunksize = 100000\n",
    "    j = 1\n",
    "    print(\"Processing: \", str(year_))\n",
    "    start_time = time.time()\n",
    "    for df in pd.read_csv(data_location + year_, chunksize=chunksize, iterator=True, skipinitialspace=True, usecols=fields):\n",
    "        \n",
    "        #remove spaces in \"column names\":\n",
    "        df = df.rename(columns={c: c.replace(' ', '') for c in df.columns}) \n",
    "        \n",
    "        print(\"Data preprocessing --> Removing URL's ...\")\n",
    "        df['article'] = df['article'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])# Data preprocessing --> remove URL \n",
    "        df['title'] = df['title'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])# Data preprocessing --> remove URL \n",
    "        \n",
    "        print(\"Data preprocessing --> Convert all to lower case ...\")\n",
    "        df = df.astype(str).apply(lambda x: x.str.lower()) # Data preprocessing --> all lower case\n",
    "        \n",
    "        print(\"Data preprocessing --> Removing Punctuation ...\")\n",
    "        df = df.astype(str).apply(lambda x: x.str.translate(str.maketrans('', '', string.punctuation))) # Data preprocessing --> remove Punctuation  \n",
    "        \n",
    "        print(\"Data preprocessing --> Removing Numbers ...\")\n",
    "        df = df.astype(str).apply(lambda x: x.str.translate(str.maketrans('', '', string.digits))) # Data preprocessing --> remove numbers          \n",
    "        \n",
    "        df.index += j # Table Index \n",
    "        \n",
    "        #------Remove \"digitized text article available\":\n",
    "        #digitized text article available,gen bullers reports,1900\n",
    "        #Digitized text of this article is not available\n",
    "        df['article'] = df['article'].apply(lambda x: regex.sub('digitized text of this article is not available', '', str(x)))\n",
    "        #------\n",
    "        \n",
    "        ## Remove non-latin characters (1)\n",
    "        print(\"Data preprocessing --> Removing non-latin characters ...\")\n",
    "        #df['article'] = df['article'].apply(lambda x: regex.sub(r'[^\\p{Latin}]', u'', str(x)))# Data preprocessing --> remove non-latin characters (1)\n",
    "        df['article'] = df['article'].apply(lambda x: regex.sub(r'[^\\p{Latin}{/^\\s*$/}]', u'', str(x)))# Data preprocessing --> remove non-latin characters (1)\n",
    "        df['title'] = df['title'].apply(lambda x: regex.sub(r'[^\\p{Latin}{/^\\s*$/}]', u'', str(x)))# Data preprocessing --> remove non-latin characters (1)\n",
    "        ## Remove non-latin characters (2)\n",
    "        df['article'] = df['article'].apply(lambda x:  re.sub(r'[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]', u'', str(x))  )# Data preprocessing --> remove non-latin characters (2)\n",
    "        df['title'] = df['title'].apply(lambda x:  re.sub(r'[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]', u'', str(x))  )# Data preprocessing --> remove non-latin characters (2)\n",
    "  \n",
    "        # Remove StopWords: [nltk]\n",
    "        print(\"Data preprocessing --> Removing StopWords ...\")\n",
    "        #from nltk import word_tokenize\n",
    "        df['title'] = df['title'].apply(lambda x: [item for item in word_tokenize(x) if item not in stop])\n",
    "        df['article'] = df['article'].apply(lambda x: [item for item in word_tokenize(x) if item not in stop])\n",
    "        \n",
    "        #Stemming: [OPTIOANL]\n",
    "        print(\"Data preprocessing --> Stemming ...\")\n",
    "        ##stemming = PorterStemmer()\n",
    "        ##df['article'] = df['article'].apply(lambda x: [stemming.stem(item) for item in x]) # Data preprocessing --> Stemming  ##// O4en' G\n",
    "        \n",
    "        # Rejoin words after Tokenization  [nltk]\n",
    "        print(\"Re-join words after Tokenization ...\")\n",
    "        df['title'] = df['title'].apply(lambda x:( \" \".join(x)))\n",
    "        df['article'] = df['article'].apply(lambda x:( \" \".join(x)))      \n",
    "        \n",
    "        df['year'] = year_.replace('.csv', '')        \n",
    "        title_ = year_.replace('.csv', '')        \n",
    "        df = df[['year', 'article', 'title']]\n",
    "\n",
    "        \n",
    "        #\"\" save to csv\n",
    "        print(\"Saving: \", title_, \".csv\")\n",
    "        path_cleaned =  '.\\\\data_cleaned\\\\'\n",
    "        df.to_csv(path_cleaned + '_cleaned_'+title_+'.csv', index = None)#, header = False)        \n",
    "        \n",
    "        j = df.index[-1] + 1 # Table Index \n",
    "        #print(\"j2 : \", j)\n",
    "        print(\"Done! --> Rows processed: \", j)\n",
    "    end_time = time.time()\n",
    "    print(\"Completed: \", str(title_) , \"--> Time: \", end_time-start_time )\n",
    "    print(\"=\"*10)\n",
    "    \n",
    "end_time_total = time.time()\n",
    "print(\"*\"*10)\n",
    "print(\"Data Preprocessing/Cleaning Done! \", \" --> Total Time: \", end_time_total-start_time_total )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
